{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2255937,"sourceType":"datasetVersion","datasetId":1357458}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nfrom itertools import combinations\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\nimport matplotlib.patches as patches\n\n\n\ndef read_kitti_image(path):\n    img = cv2.imread(path, cv2.IMREAD_COLOR)\n    if img is None:\n        raise FileNotFoundError(f\"Could not read image: {path}\")\n    return img\n\ndef read_kitti_velodyne_bin(path):\n    \"\"\"\n    Returns Nx3 float32 (x,y,z) in LiDAR frame.\n    KITTI .bin is float32 [x,y,z,intensity] per point.\n    \"\"\"\n    if not os.path.exists(path):\n        raise FileNotFoundError(f\"Could not find point cloud: {path}\")\n    pts = np.fromfile(path, dtype=np.float32).reshape(-1, 4)[:, :3]\n    return pts\n\ndef parse_calib_file(path):\n    \"\"\"\n    Parses KITTI object calib .txt into dict of name->ndarray.\n    Keys of interest: P2 (3x4), R0_rect (3x3), Tr_velo_to_cam (3x4).\n    \"\"\"\n    if not os.path.exists(path):\n        raise FileNotFoundError(f\"Could not find calib file: {path}\")\n    data = {}\n    with open(path, \"r\") as f:\n        for line in f:\n            if \":\" not in line:\n                continue\n            key, value = line.split(\":\", 1)\n            value = value.strip()\n            nums = np.fromstring(value, sep=' ')\n            if key.startswith(\"P\"):\n                data[key] = nums.reshape(3, 4)\n            elif key in (\"R0_rect\", \"R_rect\"):  # some variants use R_rect\n                data[\"R0_rect\"] = nums.reshape(3, 3)\n            elif key == \"Tr_velo_to_cam\":\n                data[key] = nums.reshape(3, 4)\n            else:\n                data[key] = nums\n    required = [\"P2\", \"R0_rect\", \"Tr_velo_to_cam\"]\n    for k in required:\n        if k not in data:\n            raise ValueError(f\"Calibration missing required key: {k}\")\n    return data\n\n\ndef read_kitti_labels(path):\n    \"\"\"\n    Reads KITTI Object Detection label file.\n    Returns a list of dicts with keys: 'type', 'bbox' = (l,t,r,b), plus other fields.\n    Skips 'DontCare' by default.\n    Format per line:\n      type, truncation, occlusion, alpha, left, top, right, bottom,\n      h, w, l, x, y, z, ry [,score]\n    \"\"\"\n    if not os.path.exists(path):\n        raise FileNotFoundError(f\"Could not find label file: {path}\")\n\n    labels = []\n    with open(path, \"r\") as f:\n        for line in f:\n            parts = line.strip().split()\n            if len(parts) < 15:\n                # Not a standard line; skip\n                continue\n            obj_type = parts[0]\n            if obj_type.lower() == \"dontcare\":\n                continue  # skip DontCare regions for drawing\n\n            trunc = float(parts[1]); occ = int(float(parts[2])); alpha = float(parts[3])\n            l, t, r, b = map(float, parts[4:8])  # 2D bbox\n            h, w, l3d = map(float, parts[8:11]) # 3D box dims (h,w,l)\n            x, y, z = map(float, parts[11:14])  # 3D location in camera coords\n            ry = float(parts[14])               # rotation around Y\n\n            lbl = {\n                \"type\": obj_type,\n                \"truncation\": trunc,\n                \"occlusion\": occ,\n                \"alpha\": alpha,\n                \"bbox\": (l, t, r, b),\n                \"dimensions\": (h, w, l3d),\n                \"location\": (x, y, z),\n                \"rotation_y\": ry\n            }\n            # optional score (for val/test predictions)\n            if len(parts) > 15:\n                try:\n                    lbl[\"score\"] = float(parts[15])\n                except:\n                    pass\n            labels.append(lbl)\n    return labels\n\n# --------------- Geometry helpers ---------------\ndef make_4x4_from_3x4(M34):\n    M44 = np.eye(4, dtype=np.float32)\n    M44[:3, :4] = M34\n    return M44\n\ndef make_4x4_from_3x3(R33):\n    M44 = np.eye(4, dtype=np.float32)\n    M44[:3, :3] = R33\n    return M44\n\ndef lidar_to_cam2_transform(calib):\n    \"\"\"\n    Build 4x4 transform: T_cam2_velo = [R0_rect 0; 0 1] @ [Tr_velo_to_cam; 0 0 0 1]\n    \"\"\"\n    R0 = calib[\"R0_rect\"]            # 3x3\n    Tr = calib[\"Tr_velo_to_cam\"]     # 3x4\n    R0_44 = make_4x4_from_3x3(R0)    # 4x4\n    Tr_44 = make_4x4_from_3x4(Tr)    # 4x4\n    return R0_44 @ Tr_44             # 4x4\n\ndef project_points_cam(pts_cam, K):\n    \"\"\"\n    pts_cam: (N,3) in camera-2 coords. K: 3x3 intrinsics.\n    Returns (u,v,z) and a valid mask (z>0).\n    \"\"\"\n    X, Y, Z = pts_cam[:,0], pts_cam[:,1], pts_cam[:,2]\n    valid = Z > 0\n    Z_safe = np.where(valid, Z, 1.0)  # avoid div-by-zero (masked anyway)\n    u = (K[0,0] * (X / Z_safe)) + K[0,2]\n    v = (K[1,1] * (Y / Z_safe)) + K[1,2]\n    uvz = np.stack([u, v, Z], axis=1)\n    return uvz, valid\n\n\ndef get_points_in_2d_bbox(pts_cam, uvz, valid, bbox, img_shape):\n    \"\"\"\n    Extract LiDAR points that fall within a 2D bounding box.\n    Returns the 3D camera coordinates of points inside the bbox.\n    \"\"\"\n    H, W = img_shape\n    l, t, r, b = bbox\n    \n    # Get projected points\n    u, v, z = uvz[:,0], uvz[:,1], uvz[:,2]\n    in_img = (u >= 0) & (u < W) & (v >= 0) & (v < H)\n    keep = valid & in_img\n    \n    # Points inside the bounding box\n    in_bbox = (u >= l) & (u <= r) & (v >= t) & (v <= b) & keep\n    \n    if np.sum(in_bbox) == 0:\n        return np.array([]).reshape(0, 3)\n    \n    return pts_cam[in_bbox]\n\ndef calculate_object_center_from_lidar(pts_in_bbox):\n    \"\"\"\n    Calculate the center position of an object using LiDAR points.\n    Uses median to be robust against outliers.\n    \"\"\"\n    if pts_in_bbox.shape[0] == 0:\n        return None\n    \n    # Use median for robustness against outliers\n    center = np.median(pts_in_bbox, axis=0)\n    return center\n\ndef calculate_object_center_from_label(label):\n    \"\"\"\n    Get object center directly from KITTI label (camera coordinates).\n    \"\"\"\n    x, y, z = label[\"location\"]\n    return np.array([x, y, z])\n\ndef calculate_distance_3d(center1, center2):\n    \"\"\"\n    Calculate Euclidean distance between two 3D points.\n    \"\"\"\n    if center1 is None or center2 is None:\n        return None\n    return np.linalg.norm(center1 - center2)\n\ndef calculate_all_distances(labels, pts_cam, uvz, valid, img_shape, method=\"lidar\"):\n    \"\"\"\n    Calculate distances between all pairs of objects.\n    \n    method: \"lidar\" - use LiDAR points to estimate centers\n            \"label\" - use ground truth centers from labels\n            \"both\"  - calculate both and compare\n    \"\"\"\n    distances = {}\n    object_centers = {}\n    \n    # Calculate centers for each object\n    for i, label in enumerate(labels):\n        obj_id = f\"{label['type']}_{i}\"\n        \n        if method in [\"lidar\", \"both\"]:\n            pts_in_bbox = get_points_in_2d_bbox(pts_cam, uvz, valid, label[\"bbox\"], img_shape)\n            lidar_center = calculate_object_center_from_lidar(pts_in_bbox)\n            object_centers[f\"{obj_id}_lidar\"] = lidar_center\n            \n        if method in [\"label\", \"both\"]:\n            label_center = calculate_object_center_from_label(label)\n            object_centers[f\"{obj_id}_label\"] = label_center\n    \n    # Calculate distances between all pairs\n    if method == \"both\":\n        # Calculate distances using both methods\n        lidar_keys = [k for k in object_centers.keys() if k.endswith(\"_lidar\")]\n        label_keys = [k for k in object_centers.keys() if k.endswith(\"_label\")]\n        \n        for i, j in combinations(range(len(labels)), 2):\n            obj1 = f\"{labels[i]['type']}_{i}\"\n            obj2 = f\"{labels[j]['type']}_{j}\"\n            \n            # LiDAR-based distance\n            lidar_dist = calculate_distance_3d(\n                object_centers.get(f\"{obj1}_lidar\"),\n                object_centers.get(f\"{obj2}_lidar\")\n            )\n            \n            # Label-based distance\n            label_dist = calculate_distance_3d(\n                object_centers.get(f\"{obj1}_label\"),\n                object_centers.get(f\"{obj2}_label\")\n            )\n            \n            distances[f\"{obj1}â†”{obj2}\"] = {\n                \"lidar\": lidar_dist,\n                \"label\": label_dist,\n                \"difference\": abs(lidar_dist - label_dist) if lidar_dist and label_dist else None\n            }\n    else:\n        # Calculate distances using single method\n        keys = list(object_centers.keys())\n        for i, j in combinations(range(len(keys)), 2):\n            key1, key2 = keys[i], keys[j]\n            dist = calculate_distance_3d(object_centers[key1], object_centers[key2])\n            distances[f\"{key1}â†”{key2}\"] = dist\n    \n    return distances, object_centers\n\ndef display_image_with_matplotlib(img, title=\"KITTI Visualization\"):\n    \"\"\"\n    Display image using matplotlib (works in Kaggle/Jupyter)\n    \"\"\"\n    # Convert BGR to RGB for matplotlib\n    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    \n    plt.figure(figsize=(15, 10))\n    plt.imshow(img_rgb)\n    plt.title(title, fontsize=16)\n    plt.axis('off')\n    plt.tight_layout()\n    plt.show()\n\ndef save_image(img, output_path):\n    \"\"\"\n    Save image to file\n    \"\"\"\n    success = cv2.imwrite(output_path, img)\n    if success:\n        print(f\"Saved image: {output_path}\")\n    else:\n        print(f\"Failed to save image: {output_path}\")\n    return success\nCLASS_COLOR = {\n    \"Car\":        (0, 255, 0),\n    \"Truck\":      (0, 150, 0),\n    \"Pedestrian\": (0, 0, 255)\n}\n\ndef color_for_class(name):\n    return CLASS_COLOR.get(name, (200, 200, 200))\n\ndef draw_labels_2d(img, labels):\n    \"\"\"\n    Draw 2D bounding boxes and class labels on image.\n    \"\"\"\n    H, W = img.shape[:2]\n    for i, det in enumerate(labels):\n        l, t, r, b = det[\"bbox\"]\n        # clip to image bounds\n        l = int(max(0, min(W-1, l)))\n        r = int(max(0, min(W-1, r)))\n        t = int(max(0, min(H-1, t)))\n        b = int(max(0, min(H-1, b)))\n\n        cls = det[\"type\"]\n        color = color_for_class(cls)\n\n        # rectangle\n        cv2.rectangle(img, (l, t), (r, b), color, 2)\n\n        # label text with background\n        label = f\"{cls}_{i}\"  # Add index for identification\n        if \"score\" in det:\n            label += f\" {det['score']:.2f}\"\n        (tw, th), baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        cv2.rectangle(img, (l, t - th - baseline - 2), (l + tw + 2, t), color, -1)\n        cv2.putText(img, label, (l + 1, t - 2), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 1, cv2.LINE_AA)\n\ndef draw_distance_lines(img, labels, object_centers, distances, pts_cam, uvz, valid, K):\n    \"\"\"\n    Draw lines between object centers with distance annotations.\n    \"\"\"\n    H, W = img.shape[:2]\n    \n    # Project 3D centers to image coordinates for drawing\n    centers_2d = {}\n    for i, label in enumerate(labels):\n        obj_id = f\"{label['type']}_{i}\"\n        \n        # Try to get LiDAR center first, fallback to label center\n        center_3d = object_centers.get(f\"{obj_id}_lidar\")\n        if center_3d is None:\n            center_3d = object_centers.get(f\"{obj_id}_label\")\n        \n        if center_3d is not None:\n            # Project to image\n            center_h = np.array([[center_3d[0], center_3d[1], center_3d[2], 1.0]])\n            uvz_center, valid_center = project_points_cam(center_3d.reshape(1, -1), K)\n            \n            if valid_center[0]:\n                u, v = int(uvz_center[0, 0]), int(uvz_center[0, 1])\n                if 0 <= u < W and 0 <= v < H:\n                    centers_2d[obj_id] = (u, v)\n                    # Draw center point\n                    cv2.circle(img, (u, v), 5, (255, 255, 255), -1)\n                    cv2.circle(img, (u, v), 5, (0, 0, 0), 2)\n    \n    # Draw distance lines\n    drawn_pairs = set()\n    for distance_key, distance_info in distances.items():\n        if isinstance(distance_info, dict):\n            # \"both\" method - use LiDAR distance if available\n            dist = distance_info.get(\"lidar\") or distance_info.get(\"label\")\n        else:\n            dist = distance_info\n            \n        if dist is None:\n            continue\n            \n        # Parse object names from key\n        obj1_name, obj2_name = distance_key.split(\"â†”\")\n        obj1_name = obj1_name.replace(\"_lidar\", \"\").replace(\"_label\", \"\")\n        obj2_name = obj2_name.replace(\"_lidar\", \"\").replace(\"_label\", \"\")\n        \n        # Avoid drawing duplicate lines\n        pair = tuple(sorted([obj1_name, obj2_name]))\n        if pair in drawn_pairs:\n            continue\n        drawn_pairs.add(pair)\n        \n        if obj1_name in centers_2d and obj2_name in centers_2d:\n            pt1 = centers_2d[obj1_name]\n            pt2 = centers_2d[obj2_name]\n            \n            # Draw line\n            cv2.line(img, pt1, pt2, (255, 255, 0), 2)\n            \n            # Draw distance text at midpoint\n            mid_x = (pt1[0] + pt2[0]) // 2\n            mid_y = (pt1[1] + pt2[1]) // 2\n            \n            distance_text = f\"{dist:.2f}m\"\n            (tw, th), baseline = cv2.getTextSize(distance_text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n            \n            # Background for text\n            cv2.rectangle(img, (mid_x - tw//2 - 2, mid_y - th - 2), \n                         (mid_x + tw//2 + 2, mid_y + 2), (0, 0, 0), -1)\n            cv2.putText(img, distance_text, (mid_x - tw//2, mid_y), \n                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2, cv2.LINE_AA)\n\n# --------------- Main ---------------\ndef main(img_path, bin_path, calib_path, label_path, index):\n    # Load data\n    img = read_kitti_image(img_path)\n    H, W = img.shape[:2]\n    pts_velo = read_kitti_velodyne_bin(bin_path)\n    calib = parse_calib_file(calib_path)\n\n    # Intrinsics from P2\n    P2 = calib[\"P2\"]                  # 3x4\n    K  = P2[:, :3]                    # 3x3\n    fx, fy, cx, cy = K[0,0], K[1,1], K[0,2], K[1,2]\n    print(f\"fx={fx:.2f}, fy={fy:.2f}, cx={cx:.2f}, cy={cy:.2f}\")\n\n    # Build LiDAR->cam2 transform\n    T_cam2_velo = lidar_to_cam2_transform(calib)  # 4x4\n\n    # Transform LiDAR points to cam2 frame\n    N = pts_velo.shape[0]\n    pts_h = np.hstack([pts_velo, np.ones((N,1), dtype=np.float32)])  # (N,4)\n    pts_cam_h = (T_cam2_velo @ pts_h.T).T\n    pts_cam = pts_cam_h[:, :3]  # (N,3)\n\n    # Project to image\n    uvz, valid = project_points_cam(pts_cam, K)\n\n    # Keep points in front & inside image bounds\n    u, v, z = uvz[:,0], uvz[:,1], uvz[:,2]\n    in_img = (u >= 0) & (u < W) & (v >= 0) & (v < H)\n    keep = valid & in_img\n\n    u_keep = u[keep].astype(np.int32)\n    v_keep = v[keep].astype(np.int32)\n    z_keep = z[keep]\n\n    # Depth-based coloring (nearâ†’far)\n    if z_keep.size > 0:\n        z_norm = (z_keep - z_keep.min()) / max(1e-6, (z_keep.max() - z_keep.min()))\n        step = max(1, len(u_keep) // 60000)  # throttle if too dense\n        for uu, vv, zn in zip(u_keep[::step], v_keep[::step], z_norm[::step]):\n            c = int(255 * (1 - zn))\n            cv2.circle(img, (uu, vv), 1, (c, 255 - c, 0), -1)\n\n    # Read labels\n    try:\n        labels = read_kitti_labels(label_path)\n        if labels:\n            #print(f\"Found {len(labels)} objects\")\n            \n            # Calculate distances between objects\n            #print(\"\\n=== Calculating Distances ===\")\n            distances, object_centers = calculate_all_distances(\n                labels, pts_cam, uvz, valid, (H, W), method=\"both\"\n            )\n            \n            # Print distance results\n            for distance_key, distance_info in distances.items():\n                if isinstance(distance_info, dict):\n                    #print(f\"{distance_key}:\")\n                    if distance_info[\"lidar\"]:\n                        #print(f\"  LiDAR-based: {distance_info['lidar']:.2f}m\")\n                        pass\n                    if distance_info[\"label\"]:\n                        #print(f\"  Label-based: {distance_info['label']:.2f}m\")\n                        pass\n                    if distance_info[\"difference\"]:\n                        #print(f\"  Difference: {distance_info['difference']:.2f}m\")\n                        pass\n                    print()\n                else:\n                    print(f\"{distance_key}: {distance_info:.2f}m\")\n            \n            # Draw labels and distances\n            draw_labels_2d(img, labels)\n            draw_distance_lines(img, labels, object_centers, distances, pts_cam, uvz, valid, K)\n            \n            #print(f\"Drew {len(labels)} 2D boxes and distances from: {label_path}\")\n        else:\n            print(\"No drawable labels found (maybe only DontCare).\")\n    except FileNotFoundError as e:\n        print(str(e))\n\n    # Display and save results\n    display_image_with_matplotlib(img, f\"KITTI {index:06d} with Distances\")\n    \n    out_path = f\"overlay_with_distances_{index:06d}.png\"\n    save_image(img, out_path)\n\n# import random\n\n# if __name__ == \"__main__\":\n#     for i in range(30):\n#         index = random.randint(0, 7499)   # random number between 0 and 2499\n#         idx_str = f\"{index:06d}\"          # zero-pad to 6 digits â†’ e.g. \"000025\"\n    \n#         img_path   = f\"/kaggle/input/kitti-3d-object-detection-dataset/training/image_2/{idx_str}.png\"\n#         bin_path   = f\"/kaggle/input/kitti-3d-object-detection-dataset/training/velodyne/{idx_str}.bin\"\n#         calib_path = f\"/kaggle/input/kitti-3d-object-detection-dataset/training/calib/{idx_str}.txt\"\n#         label_path = f\"/kaggle/input/kitti-3d-object-detection-dataset/training/label_2/{idx_str}.txt\"\n    \n#         main(img_path, bin_path, calib_path, label_path, index)\n\n# for overlaying in seq:\nif __name__ == \"__main__\":\n\n    BASE_PATH = \"/kaggle/input/kitti-3d-object-detection-dataset/training\"\n\n    img_dir   = os.path.join(BASE_PATH, \"image_2\")\n    lidar_dir = os.path.join(BASE_PATH, \"velodyne\")\n    calib_dir = os.path.join(BASE_PATH, \"calib\")\n    label_dir = os.path.join(BASE_PATH, \"label_2\")\n\n    # Get all image files in sorted order\n    img_files = sorted([\n        f for f in os.listdir(img_dir)\n        if f.endswith(\".png\")\n    ])\n\n    # Take first 300 sequential frames\n    img_files = img_files[:300]\n\n    for frame_id, img_file in enumerate(img_files):\n        idx_str = img_file.replace(\".png\", \"\")  # e.g. \"000000\"\n\n        img_path   = os.path.join(img_dir, img_file)\n        bin_path   = os.path.join(lidar_dir, f\"{idx_str}.bin\")\n        calib_path = os.path.join(calib_dir, f\"{idx_str}.txt\")\n        label_path = os.path.join(label_dir, f\"{idx_str}.txt\")\n\n        print(f\"Processing frame {frame_id} (KITTI index {idx_str})\")\n\n        main(\n            img_path=img_path,\n            bin_path=bin_path,\n            calib_path=calib_path,\n            label_path=label_path,\n            index=frame_id\n        )\n\n\n# if __name__ == \"__main__\":\n\n#     BASE_PATH = \"/kaggle/input/kitti-3d-object-detection-dataset/training\"\n\n#     img_dir   = os.path.join(BASE_PATH, \"image_2\")\n#     lidar_dir = os.path.join(BASE_PATH, \"velodyne\")\n#     calib_dir = os.path.join(BASE_PATH, \"calib\")\n#     label_dir = os.path.join(BASE_PATH, \"label_2\")\n\n#     # Sequential KITTI frames\n#     img_files = sorted([\n#         f for f in os.listdir(img_dir)\n#         if f.endswith(\".png\")\n#     ])\n\n#     NUM_FRAMES = 300        # ðŸ‘ˆ change to len(img_files) for ALL frames\n#     FPS = 10\n\n#     img_files = img_files[:NUM_FRAMES]\n\n#     video_writer = None\n\n#     for frame_id, img_file in enumerate(img_files):\n#         idx_str = img_file.replace(\".png\", \"\")\n\n#         img_path   = os.path.join(img_dir, img_file)\n#         bin_path   = os.path.join(lidar_dir, f\"{idx_str}.bin\")\n#         calib_path = os.path.join(calib_dir, f\"{idx_str}.txt\")\n#         label_path = os.path.join(label_dir, f\"{idx_str}.txt\")\n\n#         print(f\"[INFO] Processing frame {frame_id}/{NUM_FRAMES} (KITTI {idx_str})\")\n\n#         # main() MUST return the annotated frame\n#         frame = main(\n#             img_path=img_path,\n#             bin_path=bin_path,\n#             calib_path=calib_path,\n#             label_path=label_path,\n#             index=frame_id\n#         )\n\n#         # Safety check\n#         if frame is None:\n#             print(f\"[WARN] Skipping frame {frame_id}\")\n#             continue\n\n#         # Initialize VideoWriter ONCE\n#         if video_writer is None:\n#             h, w = frame.shape[:2]\n#             video_writer = cv2.VideoWriter(\n#                 \"kitti_fusion_video.mp4\",\n#                 cv2.VideoWriter_fourcc(*\"mp4v\"),\n#                 FPS,\n#                 (w, h)\n#             )\n\n#         video_writer.write(frame)\n\n#     if video_writer is not None:\n#         video_writer.release()\n\n#     print(\"âœ… Video saved as: kitti_fusion_video.mp4\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:24:55.241991Z","iopub.execute_input":"2025-12-25T10:24:55.242687Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}